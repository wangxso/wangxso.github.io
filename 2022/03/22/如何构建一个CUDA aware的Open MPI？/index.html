<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 如何构建一个CUDA aware的Open MPI？ · 非典型程序员的一生</title><meta name="description" content="如何构建一个CUDA aware的Open MPI？ - wangxso"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://wangxso.github.io/atom.xml" title="非典型程序员的一生"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="非典型程序员的一生" type="application/atom+xml">
</head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/xxx" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/wangxso" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">如何构建一个CUDA aware的Open MPI？</h1><div class="post-info">Mar 22, 2022</div><div class="post-content"><blockquote>
<p>编译后出现了错误<br>mpicc: error while loading shared libraries: &gt; libopen-pal.so.0: cannot open shared object &gt; &gt; file: No such file or directory<br>我们可以使用<br> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure><br>来解决</p>
</blockquote>
<h2 id="1-如何让Open-Mpi支持-CUDA-aware"><a href="#1-如何让Open-Mpi支持-CUDA-aware" class="headerlink" title="1. 如何让Open Mpi支持 CUDA-aware"></a>1. 如何让Open Mpi支持 CUDA-aware</h2><p>CUDA-aware 可以让你的MPI能够直接的发送和接收GPU缓存</p>
<p>Open MPI 提供了两种不同的CUDA支持</p>
<h3 id="1-使用-UCX"><a href="#1-使用-UCX" class="headerlink" title="(1). 使用 UCX."></a>(1). 使用 <a target="_blank" rel="noopener" href="https://openucx.org/">UCX</a>.</h3><p>这是你首选的一种机制，同时要确保你的ucx是使用了CUDA支持而构建的。你可以检查你的UCX是否支持CUDA。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Check <span class="keyword">if</span> ucx was built with CUDA support</span></span><br><span class="line">ucx_info -v</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">configured with: --build=powerpc64le-redhat-linux-gnu --host=powerpc64le-redhat-linux-gnu --program-prefix= --disable-dependency-tracking --prefix=/usr --exec-prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc --datadir=/usr/share --includedir=/usr/include --libdir=/usr/lib64 --libexecdir=/usr/libexec --localstatedir=/var --sharedstatedir=/var/lib --mandir=/usr/share/man --infodir=/usr/share/info --disable-optimizations --disable-logging --disable-debug --disable-assertions --enable-mt --disable-params-check --enable-cma --without-cuda --without-gdrcopy --with-verbs --with-cm --with-knem --with-rdmacm --without-rocm --without-xpmem --without-ugni --without-java</span></span><br></pre></td></tr></table></figure>
<p>然后就是构建Open MPI同时使用UCX</p>
<ul>
<li><p>获取最新的MPI版本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/open-mpi/ompi.git</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ompi</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./autogen.pl</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>编译UCX</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> build-ucx</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> build-ucx</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">../configure --prefix=&lt;ompi-install-path&gt; --with-ucx=&lt;ucx-install-path&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong> 在Open MPI 4.0 + ，可能会报”btl_uct”组件编译失败的错误，这个组件并不重要，我们可以通过以下方式禁用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./configure ... --enable-mca-no-build=btl-uct ...</span></span><br></pre></td></tr></table></figure></li>
<li><p>编译</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make install</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>运行MPI </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">mpirun -np 2 -mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 ./app</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>最近的OpenMPI版本包含一个名为“uct”的BTL组件，它可能导致在OPAL和UCM之间的Malloc钩子冲突时启用数据损坏。为了解决这个问题，使用以下替代方案之一:</p>
</blockquote>
<ul>
<li>方法1 禁止将该组件编译进来<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./configure ... --enable-mca-no-build=btl-uct ...</span></span><br></pre></td></tr></table></figure></li>
<li>方法2 禁止组件运行<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">mpirun -np 2 -mca pml ucx -mca btl ^uct -x UCX_NET_DEVICES=mlx5_0:1 ./app</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>运行调试<br>默认情况下，OpenMPI允许构建传输（BTL），这可能导致OpenMPI进度函数中的其他软件开销。为了解决此问题，您可能会尝试禁用某些BTL。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">mpirun -np 2 -mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1 ./app</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-使用内部的Open-MPI-CUDA支持"><a href="#2-使用内部的Open-MPI-CUDA支持" class="headerlink" title="(2). 使用内部的Open MPI CUDA支持"></a>(2). 使用内部的Open MPI CUDA支持</h3><p>你可以使用<code>--with-cuda=&lt;path-to-cuda&gt;</code>来构建你的MPI支持CUDA</p>
<h2 id="2-验证安装"><a href="#2-验证安装" class="headerlink" title="2. 验证安装"></a>2. 验证安装</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use ompi_info to verify cuda support <span class="keyword">in</span> Open MPI</span></span><br><span class="line"><span class="meta prompt_">shell$ </span><span class="language-bash">./ompi_info |grep <span class="string">&quot;MPI extensions</span></span></span><br></pre></td></tr></table></figure>

<p>显示</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MPI extensions: affinity, cuda, pcollreq</span><br></pre></td></tr></table></figure>

<h2 id="3-CUDA-aware支持的MPI-API"><a href="#3-CUDA-aware支持的MPI-API" class="headerlink" title="3. CUDA-aware支持的MPI API"></a>3. CUDA-aware支持的MPI API</h2><ul>
<li><p>MPI_Allgather</p>
</li>
<li><p>MPI_Allgatherv</p>
</li>
<li><p>MPI_Allreduce</p>
</li>
<li><p>MPI_Alltoall</p>
</li>
<li><p>MPI_Alltoallv</p>
</li>
<li><p>MPI_Alltoallw</p>
</li>
<li><p>MPI_Bcast</p>
</li>
<li><p>MPI_Bsend</p>
</li>
<li><p>MPI_Bsend_init</p>
</li>
<li><p>MPI_Exscan</p>
</li>
<li><p>MPI_Ibsend</p>
</li>
<li><p>MPI_Irecv</p>
</li>
<li><p>MPI_Isend</p>
</li>
<li><p>MPI_Irsend</p>
</li>
<li><p>MPI_Issend</p>
</li>
<li><p>MPI_Gather</p>
</li>
<li><p>MPI_Gatherv</p>
</li>
<li><p>MPI_Get</p>
</li>
<li><p>MPI_Put</p>
</li>
<li><p>MPI_Rsend</p>
</li>
<li><p>MPI_Rsend_init</p>
</li>
<li><p>MPI_Recv</p>
</li>
<li><p>MPI_Recv_init</p>
</li>
<li><p>MPI_Reduce</p>
</li>
<li><p>MPI_Reduce_scatter</p>
</li>
<li><p>MPI_Reduce_scatter_block</p>
</li>
<li><p>MPI_Scan</p>
</li>
<li><p>MPI_Scatter</p>
</li>
<li><p>MPI_Scatterv</p>
</li>
<li><p>MPI_Send</p>
</li>
<li><p>MPI_Send_init</p>
</li>
<li><p>MPI_Sendrecv</p>
</li>
<li><p>MPI_Ssend</p>
</li>
<li><p>MPI_Ssend_init</p>
</li>
<li><p>MPI_Win_create</p>
</li>
</ul>
<h2 id="5-CUDA-aware-不支持的API"><a href="#5-CUDA-aware-不支持的API" class="headerlink" title="5. CUDA aware 不支持的API"></a>5. CUDA aware 不支持的API</h2><ul>
<li><p>MPI_Accumulate</p>
</li>
<li><p>MPI_Compare_and_swap</p>
</li>
<li><p>MPI_Fetch_and_op</p>
</li>
<li><p>MPI_Get_Accumulate</p>
</li>
<li><p>MPI_Iallgather</p>
</li>
<li><p>MPI_Iallgatherv</p>
</li>
<li><p>MPI_Iallreduce</p>
</li>
<li><p>MPI_Ialltoall</p>
</li>
<li><p>MPI_Ialltoallv</p>
</li>
<li><p>MPI_Ialltoallw</p>
</li>
<li><p>MPI_Ibcast</p>
</li>
<li><p>MPI_Iexscan</p>
</li>
<li><p>MPI_Rget</p>
</li>
<li><p>MPI_Rput</p>
</li>
</ul>
<blockquote>
<p>参考资料 ：<br>[1]. <a target="_blank" rel="noopener" href="https://docs.open-mpi.org/en/master/networking/cuda.html?highlight=cuda">The Open MPI documentation CUDA Section</a></p>
</blockquote>
</div></article></div></main><footer><div class="paginator"><a href="/2022/04/23/%E5%A6%82%E4%BD%95%E8%AE%A9mininet%E5%9C%A8M1%E7%B3%BB%E5%88%97%E7%9A%84Macbook%E4%B8%8A%E8%BF%90%E8%A1%8C%EF%BC%9F/" class="prev">PREV</a><a href="/2022/03/05/%E5%85%B3%E4%BA%8E%E6%AD%A6%E5%A4%A7%E7%BD%91%E5%AE%89%E5%A4%8D%E8%AF%95%E7%9A%84%E4%B8%80%E4%BA%9BQA/" class="next">NEXT</a></div><div class="copyright"><p>© 2019 - 2023 <a href="https://wangxso.github.io">wangxso</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"G-51FZKV3KE9",'auto');ga('send','pageview');</script></body></html>